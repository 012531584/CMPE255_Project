{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "## kMedoids\n",
    "def assign_points_to_clusters(medoids, distances):\n",
    "    distances_to_medoids = distances[:,medoids]\n",
    "    clusters = medoids[np.argmin(distances_to_medoids, axis=1)]     # horizontal, return minimum value's index\n",
    "    clusters[medoids] = medoids\n",
    "    return clusters\n",
    "\n",
    "def compute_new_medoid(cluster, distances):\n",
    "    mask = np.ones(distances.shape)\n",
    "    mask[np.ix_(cluster,cluster)] = 0.\n",
    "    cluster_distances = np.ma.masked_array(data=distances, mask=mask, fill_value=10e9)\n",
    "    costs = cluster_distances.sum(axis=1)\n",
    "    return costs.argmin(axis=0, fill_value=10e9)\n",
    "def cluster(distances, k):\n",
    "    m = distances.shape[0] # number of points\n",
    "\n",
    "    # Pick k random medoids.\n",
    "    curr_medoids = np.array([-1]*k)\n",
    "    while not len(np.unique(curr_medoids)) == k:\n",
    "        curr_medoids = np.array([random.randint(0, m - 1) for _ in range(k)])\n",
    "    old_medoids = np.array([-1]*k) # Doesn't matter what we initialize these to.\n",
    "    new_medoids = np.array([-1]*k)\n",
    "   \n",
    "    # Until the medoids stop updating, do the following:\n",
    "    while not ((old_medoids == curr_medoids).all()):\n",
    "        # Assign each point to cluster with closest medoid.\n",
    "        clusters = []\n",
    "        clusters = assign_points_to_clusters(curr_medoids, distances)\n",
    "        # Update cluster medoids to be lowest cost point. \n",
    "        for curr_medoid in curr_medoids:\n",
    "            cluster = np.where(clusters == curr_medoid)[0]\n",
    "            new_medoids[curr_medoids == curr_medoid] = compute_new_medoid(cluster, distances)\n",
    "        old_medoids[:] = curr_medoids[:]\n",
    "        curr_medoids[:] = new_medoids[:]\n",
    "    return curr_medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def get_predict(businessid):\n",
    "    test=res_review[\"text\"][res_review.business_id==businessid]\n",
    "    l=len(test)\n",
    "    if l<60:\n",
    "        minspl=3\n",
    "        eps=0.25\n",
    "    elif l<180:\n",
    "        minspl=5\n",
    "        eps=0.25\n",
    "    elif l<500:\n",
    "        minspl=8\n",
    "        eps=0.25\n",
    "    elif l<1000:\n",
    "        minspl=10\n",
    "        eps=0.25\n",
    "    elif l<2000:\n",
    "        minspl=12\n",
    "        eps=0.25\n",
    "    else:\n",
    "        minspl=15\n",
    "        eps=0.25\n",
    "    return prediction(test,eps,minspl)\n",
    "\n",
    "def prediction(test,eps,minspl):\n",
    "    ##get data \n",
    "    p1=[]\n",
    "    for s in test:\n",
    "        p1.append(generate_candidate_phrases(s,stop))\n",
    "    words=sum(p1,[])        # transfering phrases from 2D array to 1D array\n",
    "    ##remove uncommon words\n",
    "    candidates=[]\n",
    "    for a in words:\n",
    "        if a in model.wv:\n",
    "            candidates.append(a)\n",
    "\n",
    "    ##calculate similarity\n",
    "    sim=[[0]*len(candidates) for _ in range(len(candidates))]    # create 0-based 2D array\n",
    "    for i in range(len(candidates)):\n",
    "        for j in range(i,len(candidates)):\n",
    "            d=model.wv.similarity(candidates[i],candidates[j]) \n",
    "            d = (1 if d > 1 else d)\n",
    "            d = (abs(d) if d<0 else d)\n",
    "            sim[i][j] = d\n",
    "            sim[j][i] = d\n",
    "    sim=np.array(sim)\n",
    "    dis = sim.copy()\n",
    "    dis = 1. - dis\n",
    "    \n",
    "    ##DBSCAN\n",
    "    db = DBSCAN(metric=\"precomputed\",algorithm=\"brute\",eps=eps, min_samples=minspl).fit(dis)\n",
    "    db_labels = db.labels_\n",
    "    db_core=db.core_sample_indices_\n",
    "    db_n_clusters_ = len(set(db_labels)) - (1 if -1 in db_labels else 0)    # set() to merge repeated elements\n",
    "    \n",
    "    db_result = get_result(db_n_clusters_, db_core, db_labels, candidates)\n",
    "    \n",
    "    ## kMedoids\n",
    "    Medoids_result = []\n",
    "    Medoids_points = cluster(dis, db_n_clusters_)\n",
    "    #[print(candidates[pts])for pts in Medoids_points]\n",
    "    [Medoids_result.append(candidates[pts]) for pts in Medoids_points]\n",
    "    \n",
    "    ##AffinityPropagation\n",
    "    Aff = AffinityPropagation(affinity='precomputed').fit(dis)\n",
    "    Aff_core = Aff.cluster_centers_indices_\n",
    "    Aff_n_clusters = len(set(Aff.labels_))\n",
    "    Aff_labels = Aff.labels_\n",
    "  \n",
    "    Aff_result = get_result(Aff_n_clusters, Aff_core, Aff_labels, candidates)\n",
    "    \n",
    "    \n",
    "    return db_result, Medoids_result, Aff_result,words\n",
    "\n",
    "    \n",
    "def get_result(n_clusters_, core, labels, candidates):\n",
    "    group=[[]*n_clusters_ for _ in range(n_clusters_)]   #create empty 2D array, (size : n_clusters_ * n_clusters_)\n",
    "    for i in core:\n",
    "        j=int(labels[i])\n",
    "        group[j].append(candidates[i])         # getting each cluster's phrases\n",
    "        #print(\"core_num: \", i, \" ; candidates: \", candidates[i])\n",
    "    result=[]\n",
    "    for i in range(len(group)):\n",
    "        re=group[i]\n",
    "        if len(set(re))==1:         # set to merge same phrases of one row\n",
    "            result.append(re[0])\n",
    "        else:\n",
    "            cnt = Counter(re)    # get each frequency and return most common one\n",
    "            a=cnt.most_common(1)\n",
    "            result.append(a[0][0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_eva(result):\n",
    "    total_sim = 0\n",
    "    itr_count = 1\n",
    "    for idx1 in range(len(result)):\n",
    "        for idx2 in range(idx1+1, len(result)):\n",
    "            #print(model.wv.similarity(result[idx1],result[idx2]))\n",
    "            sim = model.wv.similarity(result[idx1],result[idx2])\n",
    "            sim = (0 if sim<0 else sim)\n",
    "            itr_count +=1\n",
    "            total_sim += sim\n",
    "    #print('average similarity: ', total_sim/itr_count)\n",
    "    return total_sim/itr_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_eva(result, words):\n",
    "    if len(result)==0:   #empty result\n",
    "        return 0\n",
    "    total_freq = 0\n",
    "    words_dic = Counter(words)\n",
    "    for p in result:\n",
    "        total_freq += words_dic[p]\n",
    "    #print(\"frequency of occurance per phrase: \", total_freq/len(result))\n",
    "    return total_freq/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entire_eva(business_id):\n",
    "    sim_DB=0\n",
    "    sim_Medoid=0\n",
    "    sim_Aff=0\n",
    "    freq_DB=0\n",
    "    freq_Meoid=0\n",
    "    freq_Aff=0\n",
    "    count = 0\n",
    "    for ID_Idx in business_id.index[5:10]:    # test five business\n",
    "        business_ID = business_id[ID_Idx]\n",
    "        result_DB, result_Medoid, result_Aff, words = get_predict(business_ID)\n",
    "        \n",
    "        print(\"result_DB: \", result_DB)\n",
    "        print(\"result_Medoid: \", result_Medoid)\n",
    "        print(\"result_Aff: \", result_Aff)\n",
    "        \n",
    "        sim_DB += sim_eva(result_DB)\n",
    "        sim_Medoid += sim_eva(result_Medoid)\n",
    "        sim_Aff += sim_eva(result_Aff)\n",
    "        \n",
    "        freq_DB += freq_eva(result_DB, words)\n",
    "        freq_Meoid += freq_eva(result_Medoid, words)\n",
    "        freq_Aff += freq_eva(result_Aff, words)\n",
    "        count += 1\n",
    "    return sim_DB/count, freq_DB/count, sim_Medoid/count, freq_Meoid/count, sim_Aff/count, freq_Aff/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbscan average sim:  0.22009123048817666\n",
      "dbscan average occurance frequency:  4.1066666666666665\n",
      "kmedoid average sim:  0.07086704601624425\n",
      "kmedoid average occurance frequency:  2.0466666666666664\n",
      "AffinityPropagation average sim:  0.03758353989880901\n",
      "AffinityPropagation average occurance frequency:  1.1733333333333333\n"
     ]
    }
   ],
   "source": [
    "print('dbscan average sim: ',DB_sim_eva)\n",
    "print('dbscan average occurance frequency: ',DB_freq_eva)\n",
    "print('kmedoid average sim: ',Medoid_sim_eva)\n",
    "print('kmedoid average occurance frequency: ',Medoid_freq_eva)\n",
    "print('AffinityPropagation average sim: ',Aff_sim_eva)\n",
    "print('AffinityPropagation average occurance frequency: ',Aff_freq_eva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/randylee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: split() requires a non-empty pattern match.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_DB:  ['veggie_gyro', 'pittsburgh_airport', 'tzatziki_sauce', 'lemon_chicken_soup', 'breakfast_sandwich']\n",
      "result_Medoid:  ['specialty_sandwiches', 'food_sensitivity', 'quality_meal', 'worst_things', 'veggie_gyro']\n",
      "result_Aff:  ['awesome_service', 'reduced_price', 'particular_reason', 'minute_wait', 'menu_item', 'didnt_need', 'dont_feel', 'youre_going', 'great_service', 'timely_manner']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/randylee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: split() requires a non-empty pattern match.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_DB:  ['mexican_food', 'mama_marias', 'best_mexican_restaurant', 'enchiladas_verdes']\n",
      "result_Medoid:  ['absolutely_appalled', 'legit_mexican_food', 'chunky_salsa', 'good_service']\n",
      "result_Aff:  ['pollo_mole', 'favorite_dishes', 'good_restaurants', 'massive_meal', 'good_service', 'dont_care', 'peanut_butter_cookie', 'dont_judge', 'grilled_chicken', 'chicken_combo', 'butter_garlic_sauce', 'expensive_places', 'menu_items', 'good_price', 'great_deal', 'dozen_items', 'good_stuff', 'wait_staff', 'free_refill', 'white_meat', 'days_later', 'absolutely_delicious', 'good_things', 'family_restaurant', 'closing_time', 'saturday_night', 'service_wasnt', 'little_slow']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/randylee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: split() requires a non-empty pattern match.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_DB:  ['coconut_whipped_cream', 'acai_bowl', 'beet_juice']\n",
      "result_Medoid:  ['obviously_dont_care', 'grain_bowl', 'acai_bowl']\n",
      "result_Aff:  ['good_experience', 'dont_want', 'havent_seen', 'gluten_free', 'place_popular', 'dont_know', 'recently_opened', 'super_friendly', 'people_serving', 'soooooooo_good', 'quick_bite', 'menu_items', 'hard_pressed', 'youre_going', 'dont_think']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/randylee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: split() requires a non-empty pattern match.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_DB:  ['vegan_options', 'best_donut', 'dont_know', 'food_took', 'saturday_morning', 'brunch_menu']\n",
      "result_Medoid:  ['forbidden_fruit', 'brunch_spots', 'vegan_friendly', 'table_minutes_later', 'friends_plate', 'smaller_group']\n",
      "result_Aff:  ['vegan_options', 'high_quality', 'gluten_free', 'place_feels', 'dance_floor', 'super_nice', 'probably_worth', 'probably_wont', 'cold_water', 'didnt_bother', 'great_service', 'normal_breakfast', 'suggest_getting', 'fantastic_time']\n",
      "result_DB:  []\n",
      "result_Medoid:  []\n",
      "result_Aff:  ['tried_different_appetizers', 'lightly_breaded', 'place_started', 'large_entree', 'honey_mustard', 'taco_place', 'food_quality', 'absolutely_delicious', 'totally_missed']\n",
      "CPU times: user 14.6 s, sys: 167 ms, total: 14.7 s\n",
      "Wall time: 14.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/randylee/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: split() requires a non-empty pattern match.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "%time DB_sim_eva, DB_freq_eva, Medoid_sim_eva, Medoid_freq_eva, Aff_sim_eva, Aff_freq_eva = entire_eva(business_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
